{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/noether/s0/aqd5773/representations/notebooks/2a_generalizability_gfa.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnoether.matse.psu.edu/noether/s0/aqd5773/representations/notebooks/2a_generalizability_gfa.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnoether.matse.psu.edu/noether/s0/aqd5773/representations/notebooks/2a_generalizability_gfa.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnoether.matse.psu.edu/noether/s0/aqd5773/representations/notebooks/2a_generalizability_gfa.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m \u001b[39mimport\u001b[39;00m get_elem_count, alt_read_gfa_dataset, PTR, check_cuda, get_metrics, image, pymatgen_comp, special_formatting, data_generator_gfa\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnoether.matse.psu.edu/noether/s0/aqd5773/representations/notebooks/2a_generalizability_gfa.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mencoder\u001b[39;00m \u001b[39mimport\u001b[39;00m Encoder1D, EncoderDNN, Encoder\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnoether.matse.psu.edu/noether/s0/aqd5773/representations/notebooks/2a_generalizability_gfa.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m/noether/s0/aqd5773/representations/modules/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0.1.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m encoder\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m function\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m plotting_functions\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m representation_schemes\n",
      "File \u001b[0;32m/noether/s0/aqd5773/representations/modules/function.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPTR_Encoder.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msaved_models_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     PTR_encoder \u001b[39m=\u001b[39m  joblib\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00msaved_models_path\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mtype\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilename\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNo file found!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/joblib/numpy_pickle.py:587\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m                 \u001b[39mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 587\u001b[0m             obj \u001b[39m=\u001b[39m _unpickle(fobj, filename, mmap_mode)\n\u001b[1;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/joblib/numpy_pickle.py:506\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 506\u001b[0m     obj \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    507\u001b[0m     \u001b[39mif\u001b[39;00m unpickler\u001b[39m.\u001b[39mcompat_mode:\n\u001b[1;32m    508\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has been generated with a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mjoblib version less than 0.10. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    510\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mPlease regenerate this pickle file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m                       \u001b[39m%\u001b[39m filename,\n\u001b[1;32m    512\u001b[0m                       \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1214\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/pickle.py:1590\u001b[0m, in \u001b[0;36m_Unpickler.load_reduce\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1588\u001b[0m args \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39mpop()\n\u001b[1;32m   1589\u001b[0m func \u001b[39m=\u001b[39m stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m-> 1590\u001b[0m stack[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/torch/storage.py:222\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/torch/serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m    712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> 713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/torch/serialization.py:930\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    928\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    929\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m--> 930\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    932\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    934\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/torch/serialization.py:876\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    872\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    873\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    874\u001b[0m     \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m    875\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m--> 876\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    877\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    879\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[root_key]\n\u001b[1;32m    880\u001b[0m \u001b[39mif\u001b[39;00m view_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/noether/s0/aqd5773/anaconda3/envs/representations/lib/python3.10/site-packages/torch/serialization.py:155\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n\u001b[0;32m--> 155\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_UntypedStorage(obj\u001b[39m.\u001b[39;49mnbytes(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(location))\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mcuda(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "from decimal import Decimal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "from modules.function import get_elem_count, alt_read_gfa_dataset, PTR, check_cuda, get_metrics, image, pymatgen_comp, special_formatting, data_generator_gfa\n",
    "from modules.encoder import Encoder1D, EncoderDNN, Encoder\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import joblib\n",
    "from modules.representation_schemes import get_vectorized_featues, get_atomic_number_features, get_pettifor_features, get_modified_pettifor_features, get_random_features, get_random_features_dense, random_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "cuda = check_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfa_dataset_file = 'gfa_dataset.txt'\n",
    "z_row_column_file = 'Z_row_column.txt'\n",
    "element_property_file = 'element_property.txt'\n",
    "common_path = \"Files_from_GTDL_paper/{}\" \n",
    "gfa_dataset = pickle.load(open(common_path.format(gfa_dataset_file), 'rb'))  \n",
    "RC = pickle.load(open(common_path.format(z_row_column_file), 'rb')) \n",
    "new_index=[int(i[4]) for i in RC]#new order \n",
    "Z_row_column = pickle.load(open(common_path.format(z_row_column_file), 'rb'))\n",
    "[property_name_list,property_list,element_name,_]=pickle.load(open(common_path.format(element_property_file), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_gfa, y, p = alt_read_gfa_dataset()\n",
    "count_dict = get_elem_count(comps_gfa)\n",
    "count_df = pd.DataFrame.from_dict(count_dict, orient='index')\n",
    "count_df.reset_index(inplace=True)\n",
    "count_df.rename({'index':'element',0:'count'}, axis=1, inplace=True)\n",
    "count_df['percent'] = [np.round(count_df.loc[i,'count']/len(comps_gfa)*100,2) for i in range(count_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_d_elements(lst,count,format:str):\n",
    "    if format in ['atomic','pettifor','mod_pettifor']:\n",
    "        if format == 'atomic':\n",
    "            rep,_ = get_atomic_number_features(lst)\n",
    "        elif format == 'pettifor':\n",
    "            rep,_ = get_pettifor_features(lst)\n",
    "        elif format == 'mod_pettifor':\n",
    "            rep,_ = get_modified_pettifor_features(lst)\n",
    "        if len(count.shape) != 2:\n",
    "            count = count.reshape(-1,1)\n",
    "        with_count = rep.squeeze()*count\n",
    "    return with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_elements = [image(count_df['element'][i]+str(count_df['percent'][i]), normalized = False) for i in range(count_df.shape[0])]\n",
    "atomic_elements = one_d_elements(count_df['element'], count_df['percent'].values, format = 'atomic')\n",
    "pettifor_elements = one_d_elements(count_df['element'], count_df['percent'].values, format = 'pettifor')\n",
    "mod_pettifor_elements = one_d_elements(count_df['element'], count_df['percent'].values, format = 'mod_pettifor')\n",
    "norm_dist_ptr= []\n",
    "norm_dist_atomic = []\n",
    "norm_dist_pet = []\n",
    "norm_dist_mod_pet = []\n",
    "for i in range(len(ptr_elements)):\n",
    "    norm_dist_ptr.append(np.linalg.norm(ptr_elements[i] - ptr_elements))\n",
    "    norm_dist_atomic.append(np.linalg.norm(atomic_elements[i] - atomic_elements))\n",
    "    norm_dist_pet.append(np.linalg.norm(pettifor_elements[i] - pettifor_elements))\n",
    "    norm_dist_mod_pet.append(np.linalg.norm(mod_pettifor_elements[i] - mod_pettifor_elements))\n",
    "norm_dist_pet = np.array(norm_dist_pet)\n",
    "norm_dist_atomic = np.array(norm_dist_atomic)\n",
    "norm_dist_ptr = np.array(norm_dist_ptr)\n",
    "norm_dist_mod_pet = np.array(norm_dist_mod_pet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>element</th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>W</td>\n",
       "      <td>380</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>V</td>\n",
       "      <td>500</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Nb</td>\n",
       "      <td>1078</td>\n",
       "      <td>5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mo</td>\n",
       "      <td>1222</td>\n",
       "      <td>5.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ti</td>\n",
       "      <td>1930</td>\n",
       "      <td>9.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Co</td>\n",
       "      <td>2294</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cr</td>\n",
       "      <td>2714</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fe</td>\n",
       "      <td>5302</td>\n",
       "      <td>25.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ni</td>\n",
       "      <td>6112</td>\n",
       "      <td>29.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Al</td>\n",
       "      <td>7084</td>\n",
       "      <td>33.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   element  count  percent\n",
       "37       W    380     1.82\n",
       "9        V    500     2.40\n",
       "21      Nb   1078     5.17\n",
       "22      Mo   1222     5.86\n",
       "8       Ti   1930     9.25\n",
       "13      Co   2294    10.99\n",
       "10      Cr   2714    13.00\n",
       "12      Fe   5302    25.40\n",
       "14      Ni   6112    29.29\n",
       "4       Al   7084    33.94"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df['percent'] = [np.round(count_df.loc[i,'count']/len(comps_gfa)*100,2) for i in range(count_df.shape[0])]\n",
    "elements = ['Al','Ni','Fe','Cr','Co','Ti','Mo','Nb','V','W']\n",
    "count_df[[el in elements for el in count_df['element'].values]].sort_values('percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances for training and test compositions for LOEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_inds = np.arange(0,len(comps_gfa),2)\n",
    "unique_comps = [comps_gfa[i] for i in selected_inds]\n",
    "unqiue_elements = count_df['element'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_dist(unqiue_elements = unqiue_elements, unique_comps = unique_comps, to_element = False):\n",
    "    dist_dict = {}\n",
    "    keys = ['PTR','atomic','pettifor','mod_pettifor']\n",
    "    for method in keys:\n",
    "        el_dict = {}\n",
    "        for unq in unqiue_elements:\n",
    "            training_set = []\n",
    "            test_set = []\n",
    "            for comp in unique_comps:\n",
    "                if unq in comp.get_el_amt_dict().keys():\n",
    "                    test_set.append(comp)\n",
    "                else:\n",
    "                    training_set.append(comp)\n",
    "            element_comp = [unq+str(100)]\n",
    "            avg_euclid_dist = 0\n",
    "            if method == 'PTR':\n",
    "                train = np.array([image(special_formatting(i)) for i in training_set])\n",
    "                test = np.array([image(special_formatting(i)) for i in test_set])\n",
    "                element = np.array([image(elem) for elem in element_comp])\n",
    "            elif method == 'atomic':\n",
    "                train,_ = get_atomic_number_features(training_set)\n",
    "                test,_ = get_atomic_number_features(test_set)\n",
    "                element,_ = get_atomic_number_features(element_comp)\n",
    "            elif method == 'pettifor':\n",
    "                train,_ = get_pettifor_features(training_set)\n",
    "                test,_ = get_pettifor_features(test_set)\n",
    "                element,_ = get_pettifor_features(element_comp)\n",
    "            elif method == 'mod_pettifor':\n",
    "                train,_ = get_modified_pettifor_features(training_set)\n",
    "                test,_ = get_modified_pettifor_features(test_set)\n",
    "                element,_ = get_modified_pettifor_features(element_comp)\n",
    "            if not to_element:\n",
    "                for x in test:\n",
    "                    avg_euclid_dist += np.linalg.norm(train-x,axis=1).mean()\n",
    "                avg_euclid_dist = avg_euclid_dist/len(test)\n",
    "                \n",
    "            else:\n",
    "                avg_euclid_dist += np.linalg.norm(train-element).mean()\n",
    "\n",
    "            el_dict[unq] = avg_euclid_dist\n",
    "        \n",
    "        dist_dict[method] = el_dict\n",
    "    return dist_dict\n",
    "\n",
    "\n",
    "types = ['element','set']\n",
    "for t in types:\n",
    "    filename = 'misc/LOEO_distances_holdout_{}.json'.format(t)\n",
    "    arg = False\n",
    "    if t == 'element':\n",
    "        arg = True\n",
    "    if not os.path.exists(filename):\n",
    "        distance_df = get_euclidean_dist(to_element=arg) \n",
    "        with open(filename,'w') as fid:\n",
    "            json.dump(distance_df,fid)\n",
    "    else:\n",
    "        if t == 'element':\n",
    "            with open(filename,'r') as fid:\n",
    "                distance_df_elem = json.load(fid)\n",
    "        else:\n",
    "            with open(filename,'r') as fid:\n",
    "                distance_df_set = json.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame.from_dict(distance_df_elem)\n",
    "np.argsort(test_df['atomic'].values) == np.argsort(test_df['pettifor'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.242640687119285\n",
      "5.656854249492381\n",
      "7.0710678118654755\n",
      "8.48528137423857\n",
      "2.8284271247461903\n",
      "4.242640687119285\n",
      "5.656854249492381\n",
      "7.0710678118654755\n",
      "1.4142135623730951\n",
      "2.8284271247461903\n",
      "4.242640687119285\n",
      "5.656854249492381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.24264069, 5.65685425, 7.07106781, 8.48528137],\n",
       "       [2.82842712, 4.24264069, 5.65685425, 7.07106781],\n",
       "       [1.41421356, 2.82842712, 4.24264069, 5.65685425]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "a = np.array([[1,2],[2,3],[3,4]])\n",
    "b = np.array([[4,5],[5,6],[6,7],[7,8]])\n",
    "for x in a:\n",
    "    for y in b:\n",
    "        print(np.linalg.norm(x-y))\n",
    "distance_matrix(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] 6.363961030678928\n",
      "[2 3] 4.949747468305833\n",
      "[3 4] 3.5355339059327378\n"
     ]
    }
   ],
   "source": [
    "for x in a:\n",
    "    print(x,np.linalg.norm(b-x, axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260629/291367725.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train = np.array(comps_gfa)[train_inds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split file written!\n"
     ]
    }
   ],
   "source": [
    "elements = ['Al','Ni','Fe','Cr','Co','Ti','Mo','Nb','V','W']\n",
    "filename = 'misc/gfa_gen_splits.json'\n",
    "if os.path.exists(filename):\n",
    "    with open(filename,'rb') as fid:\n",
    "        gfa_gen_dict = json.load(fid)\n",
    "        print('Split file loaded')\n",
    "else:\n",
    "\n",
    "    gfa_gen_dict = {}\n",
    "    for el in elements:\n",
    "        train_inds, test_inds,all_cv_train, all_cv_test = [],[],[],[]\n",
    "        for i,c in enumerate(comps_gfa):\n",
    "            if el in c.get_el_amt_dict().keys():\n",
    "                train_inds.append(i)\n",
    "            else:\n",
    "                test_inds.append(i)\n",
    "        X_train = np.array(comps_gfa)[train_inds]\n",
    "        kf = KFold(n_splits=5)\n",
    "        for tr, ts in kf.split(X_train):\n",
    "            all_cv_train.append(tr.tolist())\n",
    "            all_cv_test.append(ts.tolist())\n",
    "        gfa_gen_dict[el] = {'train':train_inds,'test':test_inds, 'cv_train':all_cv_train, 'cv_test':all_cv_test}\n",
    "    \n",
    "    with open(filename,'w') as fid:\n",
    "        json.dump(gfa_gen_dict, fid)\n",
    "        print('Split file written!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1D_features_gfa(k:str):\n",
    "    comp_gfa, y, p = alt_read_gfa_dataset()\n",
    "    y = np.array(y).reshape(-1,1).astype('float32')\n",
    "    p = np.array(p).reshape(-1,1).astype('float32')\n",
    "    if k not in ['atomic','pettifor','mod_pettifor','random']:\n",
    "        print('Unsupported format')\n",
    "        return None, None, None\n",
    "    else:\n",
    "        if k == 'atomic':\n",
    "            comp, at_order  = get_atomic_number_features(comp_gfa)\n",
    "        elif k == 'pettifor':\n",
    "            comp, _  = get_pettifor_features(comp_gfa)\n",
    "        elif k == 'mod_pettifor':\n",
    "            comp, _  = get_modified_pettifor_features(comp_gfa)\n",
    "        elif k == 'random':\n",
    "            comp,_ = get_random_features(comp_gfa, random_order)\n",
    "        return comp, y, p\n",
    "\n",
    "def get_dense_features_gfa():\n",
    "    comp_gfa, y, p = alt_read_gfa_dataset()\n",
    "    y = np.array(y).reshape(-1,1).astype('float32')\n",
    "    p = np.array(p).reshape(-1,1).astype('float32') \n",
    "    comp,_ = get_random_features_dense(comp_gfa, random_order)\n",
    "    return comp, y, p\n",
    "\n",
    "def get_ptr_features_gfa(gfa_dataset=gfa_dataset):\n",
    "\n",
    "    gfa_i=[]\n",
    "    gfa_a=[]\n",
    "    gfa_b=[]\n",
    "    gfa_c=[]\n",
    "    to_discard = ['Rf','Db','Sg','Bh','Hs']\n",
    "    for i in  gfa_dataset:\n",
    "        tx_gfa=re.findall('\\[[a-c]?\\]', i)\n",
    "        tx1_element=re.findall('[A-Z][a-z]?', i)#[B, Fe, P,No]\n",
    "        if len(set(tx1_element).intersection(set(to_discard))) == 0:      \n",
    "            gfa_i.extend(tx_gfa)\n",
    "            if tx_gfa[0]=='[a]':\n",
    "                gfa_a.append(gfa_dataset.index(i))\n",
    "            elif tx_gfa[0]=='[b]':\n",
    "                gfa_b.append(gfa_dataset.index(i)) \n",
    "            else:\n",
    "                gfa_c.append(gfa_dataset.index(i))\n",
    "        \n",
    "    gfa_data_form=[]\n",
    "    gfa_data_form_p = []\n",
    "    gfa_data_form_b=[]\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#map raw data to 2-D image using PTR\n",
    "    for i in gfa_a:\n",
    "        x,p,y = PTR(gfa_dataset[i])\n",
    "        gfa_data_form=gfa_data_form+x\n",
    "        gfa_data_form_p = gfa_data_form_p+p\n",
    "        gfa_data_form_b=gfa_data_form_b+y\n",
    "    for i in gfa_c:\n",
    "        x,p,y = PTR(gfa_dataset[i])\n",
    "        gfa_data_form=gfa_data_form+x\n",
    "        gfa_data_form_p = gfa_data_form_p+p\n",
    "        gfa_data_form_b=gfa_data_form_b+y \n",
    "    for i in gfa_b:\n",
    "        x,p,y = PTR(gfa_dataset[i])\n",
    "        gfa_data_form=gfa_data_form+x\n",
    "        gfa_data_form_p = gfa_data_form_p+p\n",
    "        gfa_data_form_b=gfa_data_form_b+y\n",
    "\n",
    "    X_all = np.array(gfa_data_form).reshape(-1, 1,9, 18).astype('float32') \n",
    "    y_all = np.array(gfa_data_form_b).reshape(-1,1).astype('float32')\n",
    "    p_all = np.array(gfa_data_form_p).reshape(-1,1).astype('float32')\n",
    "    return X_all, y_all, p_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveloc = 'saved_models/LOEO_Encoders'\n",
    "if not os.path.exists(saveloc):\n",
    "    os.makedirs(f'{saveloc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010243654251098633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dfc4805f274beb8e0a73662d72f1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense : Element Al, Fold 0, Epoch : 1, Loss : 59.46879905462265\n",
      "dense : Element Al, Fold 0, Epoch : 500, Loss : 5.203987020999193\n",
      "dense : Element Al, Fold 0, Epoch : 1000, Loss : 4.191072904504836\n",
      "dense : Element Al, Fold 0, Epoch : 1500, Loss : 3.908863263204694\n",
      "dense : Element Al, Fold 0, Epoch : 2000, Loss : 3.772492978256196\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009945869445800781,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97609669b5e846caab27b5669eb72af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense : Element Al, Fold 1, Epoch : 1, Loss : 60.659728050231934\n",
      "dense : Element Al, Fold 1, Epoch : 500, Loss : 4.441414857748896\n",
      "dense : Element Al, Fold 1, Epoch : 1000, Loss : 3.7315723549108952\n",
      "dense : Element Al, Fold 1, Epoch : 1500, Loss : 3.4324506116099656\n",
      "dense : Element Al, Fold 1, Epoch : 2000, Loss : 3.312455483013764\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011800527572631836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48af860bab8941aba30b628fb6954b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense : Element Al, Fold 2, Epoch : 1, Loss : 57.727927446365356\n",
      "dense : Element Al, Fold 2, Epoch : 500, Loss : 4.739594083279371\n",
      "dense : Element Al, Fold 2, Epoch : 1000, Loss : 3.9628490114118904\n",
      "dense : Element Al, Fold 2, Epoch : 1500, Loss : 4.0655128138605505\n",
      "dense : Element Al, Fold 2, Epoch : 2000, Loss : 3.1386625161394477\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010319948196411133,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68985c95b25428e91c679d6804861a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense : Element Al, Fold 3, Epoch : 1, Loss : 51.827803552150726\n",
      "dense : Element Al, Fold 3, Epoch : 500, Loss : 5.852062711492181\n",
      "dense : Element Al, Fold 3, Epoch : 1000, Loss : 4.74813527520746\n",
      "dense : Element Al, Fold 3, Epoch : 1500, Loss : 4.108698435593396\n"
     ]
    }
   ],
   "source": [
    "result_file = 'results/gfa_LOEO_stats.json'\n",
    "methods = ['dense','atomic','pettifor','mod_pettifor','random','PTR']\n",
    "batch = 64\n",
    "num_iterations = 2000\n",
    "log_interval = int(5e2)\n",
    "results_dict = {}\n",
    "for method in methods:\n",
    "    results_dict[method] = {}\n",
    "    if method == 'dense':\n",
    "        X, y , p = get_dense_features_gfa()    \n",
    "    elif method in ['atomic','pettifor','mod_pettifor','random']:\n",
    "        X,y,p = get_1D_features_gfa(method)     \n",
    "    elif method == 'PTR':\n",
    "        X,y,p = get_ptr_features_gfa()\n",
    "    for k in gfa_gen_dict.keys():\n",
    "        results_dict[method][k] = {}\n",
    "        f1_max = 0\n",
    "        best_fold_model = 0\n",
    "        LOEO_dict = {'Fold_stats':{}}\n",
    "        test_inds = gfa_gen_dict[k]['test']\n",
    "        X_test = X[test_inds]\n",
    "        y_test = y[test_inds]\n",
    "        p_test = p[test_inds]\n",
    "        if X_test.dtype != torch.float32:\n",
    "            X_test = torch.from_numpy(X_test)\n",
    "        if p_test.dtype != torch.float32:\n",
    "            p_test = torch.from_numpy(p_test)\n",
    "        if cuda:\n",
    "            X_test = X_test.cuda()\n",
    "            p_test = p_test.cuda()\n",
    "        cv_train_inds, cv_test_inds = gfa_gen_dict[k]['cv_train'],gfa_gen_dict[k]['cv_test']\n",
    "        for i in range(len(cv_train_inds)):\n",
    "            fold_train_inds, fold_test_inds = cv_train_inds[i], cv_test_inds[i]\n",
    "            X_train_fold,y_train_fold ,p_train_fold  = X[fold_train_inds], y[fold_train_inds], p[fold_train_inds]\n",
    "            X_test_fold, y_test_fold, p_test_fold = X[fold_test_inds], y[fold_test_inds], p[fold_test_inds]\n",
    "            Xy = [(X_train_fold[i],y_train_fold[i],p_train_fold[i]) for i in range(len(y_train_fold))]\n",
    "            train_loader = DataLoader(Xy, batch_size = batch , shuffle=True)\n",
    "            if X_test_fold.dtype != torch.float32:\n",
    "                X_test_fold = torch.from_numpy(X_test_fold)\n",
    "            if p_test_fold.dtype != torch.float32:\n",
    "                p_test_fold = torch.from_numpy(p_test_fold)\n",
    "            if method == 'dense':\n",
    "                encoder = EncoderDNN(X_train_fold.shape[-1],3,42,1)\n",
    "            elif method in ['atomic','pettifor','mod_pettifor','random']:\n",
    "                encoder = Encoder1D(1,1)\n",
    "            elif method == 'PTR':\n",
    "                encoder = Encoder(1,1)\n",
    "            \n",
    "            if cuda:\n",
    "                encoder = encoder.cuda()\n",
    "                X_test_fold, p_test_fold = X_test_fold.cuda(), p_test_fold.cuda()\n",
    "            e_optimizer = optim.Adam(encoder.parameters(),lr = 2e-4)\n",
    "            for iter in tqdm.notebook.tqdm(range(num_iterations)):\n",
    "                train_loss = 0.0\n",
    "                for data in train_loader:\n",
    "                    X_t,y_t,p_t = data\n",
    "                    if cuda:\n",
    "                        X_t = X_t.cuda()\n",
    "                        y_t = y_t.cuda()\n",
    "                        p_t = p_t.cuda()\n",
    "                    e_optimizer.zero_grad()\n",
    "                    target = encoder(X_t,p_t)\n",
    "                    if cuda:\n",
    "                        target = target.cuda()\n",
    "                    e_error = torch.nn.BCELoss()(target,y_t)\n",
    "                    e_error.backward(retain_graph=True)\n",
    "                    e_optimizer.step()\n",
    "                    train_loss += e_error.cpu().item()\n",
    "                if iter == 0 or (iter + 1) % log_interval == 0:  \n",
    "                    print('{} : Element {}, Fold {}, Epoch : {}, Loss : {}'.format(method,k,i,iter+1,train_loss))\n",
    "            spec_saveloc = os.path.join(saveloc,method)\n",
    "            if not os.path.exists(spec_saveloc):\n",
    "                os.makedirs(f'{spec_saveloc}')\n",
    "            joblib.dump(encoder,os.path.join(spec_saveloc,'LOEOEncoder_{}_fold{}.pt'.format(k,i)))\n",
    "            y_predict_fold = (encoder(X_test_fold,p_test_fold)).to('cpu').detach().numpy()\n",
    "            metrics_fold = get_metrics(y_test_fold,np.round(y_predict_fold))\n",
    "            LOEO_dict['Fold_stats'][i] = metrics_fold\n",
    "            y_predict = (encoder(X_test,p_test)).to('cpu').detach().numpy()\n",
    "            metrics = get_metrics(y_test,np.round(y_predict))\n",
    "            f1_predict = metrics[3]\n",
    "            if f1_predict>f1_max:\n",
    "                f1_max = f1_predict\n",
    "                best_fold_model = i\n",
    "                LOEO_dict['Best_f1'] = f1_max\n",
    "                LOEO_dict['Best_fold'] = best_fold_model\n",
    "        results_dict[method][k] = LOEO_dict\n",
    "        if os.path.exists(result_file):\n",
    "            with open(result_file,'rb') as fid:\n",
    "                data_file = json.load(fid)\n",
    "            updated_file = data_file|results_dict\n",
    "            with open(result_file,'w') as f:\n",
    "                json.dump(updated_file,f)\n",
    "        else:\n",
    "            with open(result_file, 'w') as f:\n",
    "                json.dump(results_dict, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('representations': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d6d4f329abd04bfe6379b099819ba9ed35c0bee691259b7aa14222222f8ecb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
